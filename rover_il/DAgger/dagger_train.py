# rover_il/DAgger/dagger_train.py
# --------------------------------------------------------------------- #
#  Build student PPO + BC + SimpleDAggerTrainer                         #
# --------------------------------------------------------------------- #
import os, numpy as np, torch
from stable_baselines3 import PPO
from imitation.algorithms.bc import BC
from imitation.algorithms.dagger import SimpleDAggerTrainer

from rover_il.DAgger.wrappers import STUDENT_KEYS           # just for log/debug
from rover_il.learning.models.models import (
    SB3_GaussianNeuralNetworkConvResNet,
)

# --------------------------------------------------------------------- #
def build_dagger(
    *,                                           # force keyword args
    vec_student,                                 # DummyVecEnv (flattened)
    vec_expert,                                  # DummyVecEnv (flattened→np)
    expert_policy,                               # SkrlExpertPolicy wrapper
    logdir: str,
    device: str = "cuda:0",
    total_steps: int = 100_000,
):
    """
    Returns (dagger_trainer, sb3_student, run_fn) where
      • `dagger_trainer.train(total_timesteps)` performs the aggregation loop
      • `run_fn()` is a convenience wrapper that runs and then saves the student
    """

    # ------------------------------------------------------------------ #
    # Student algorithm (PPO)                                        #
    # ------------------------------------------------------------------ #
    student_alg = PPO(
        policy            = "MultiInputPolicy",
        env               = vec_student,
        device            = device,
        verbose           = 1,
        policy_kwargs     = dict(
            features_extractor_class = SB3_GaussianNeuralNetworkConvResNet,
            features_extractor_kwargs = dict(
                mlp_layers         = [256, 160, 128],
                mlp_activation     = "leaky_relu",
                encoder_layers     = [8, 16, 32, 64],
                encoder_activation = ("relu", "leaky_relu"),
                # Mapping info (generated by FlattenPolicyObs)
                key_slices         = vec_student.key_slices,
                key_shapes         = vec_student.key_shapes,
            ),
        ),
    )

    # ------------------------------------------------------------------ #
    # Behaviour-Cloning helper used inside DAgger                    #
    # ------------------------------------------------------------------ #
    bc_trainer = BC(
        observation_space = vec_student.observation_space,
        action_space      = vec_student.action_space,
        policy            = student_alg.policy,
        device            = device,
        rng               = np.random.default_rng(0),
    )

    # ------------------------------------------------------------------ #
    # Simple DAgger trainer                                          #
    # ------------------------------------------------------------------ #
    dagger = SimpleDAggerTrainer(
        venv          = vec_expert,
        expert_policy = expert_policy,
        bc_trainer    = bc_trainer,
        scratch_dir   = logdir,
        rng           = np.random.default_rng(0),
        beta_schedule = lambda _: 1.0,       # Always query expert
    )

    # one-liner helper for your train.py
    def run():
        print(f"[DAgger] collecting + training for {total_steps:,} steps …")
        dagger.train(total_timesteps = total_steps, bc_train_kwargs={"progress": True})
        path = os.path.join(logdir, "student_final")
        student_alg.save(path)
        print(f"[DAgger] student saved → {path}")

    return dagger
